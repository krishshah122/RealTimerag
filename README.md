# ğŸš€ Real-Time RAG

### _Ask questions over a live stream of operational issuesâ€”answers grounded in up-to-date context, no restarts._

**Python â€¢ FastAPI â€¢ React â€¢ LangGraph â€¢ FAISS â€¢ Kafka â€¢ Groq**

**Purpose-Built Hybrid Retrieval with Real-Time Ingestion and Async Indexing**

Features â€¢ Architecture â€¢ Quick Start â€¢ API â€¢ Contributing

---

## ğŸ“– Table of Contents

- [Overview](#-overview)
- [Key Features](#-key-features)
- [Architecture](#-architecture)
- [Tech Stack](#ï¸-tech-stack)
- [Quick Start](#-quick-start)
- [Usage Guide](#-usage-guide)
- [API Documentation](#-api-documentation)
- [Project Structure](#-project-structure)
- [Pushing to GitHub](#-pushing-to-github)
- [Contributing](#-contributing)
- [License](#-license)
- [Acknowledgments](#-acknowledgments)

---

## ğŸŒŸ Overview

**Real-Time RAG** is a full-stack retrieval-augmented generation system that keeps your knowledge base **live**. Operational issues (alerts, incidents, delays) flow in via a message stream, get indexed in a vector store without blocking the API, and users ask natural-language questionsâ€”getting answers grounded **only** in the retrieved documents.

### ğŸ¯ The Problem

Operations and support teams face three critical challenges:

- ğŸ“š **Information overload** â€” Alerts, logs, and tickets pour in from many sources; finding "what's going on" means scanning dashboards and tools.
- ğŸ”„ **Stale RAG** â€” Classic RAG assumes a static corpus; new issues require re-indexing or server restarts to show up in answers.
- â±ï¸ **Slow ingest under load** â€” If every logged issue triggers embedding + indexing in the API, latency spikes and throughput drops when volume grows.

### ğŸ’¡ Our Solution

Real-Time RAG provides:

- **ğŸ“¥ Async ingestion** â€” API publishes to Kafka and returns immediately; a background consumer does embedding and indexing so the system stays fast and scalable.
- **ğŸ” Hybrid retrieval** â€” Dense (FAISS) + sparse (BM25) with RRF fusion and rerank so answers use the best mix of semantic and keyword match.
- **ğŸ“¡ Always-fresh context** â€” Retrieve step loads the latest index from disk on every query, so new issues appear in answers without restarting anything.
- **ğŸ¯ Grounded answers** â€” The LLM is prompted to use **only** the provided context, so responses are traceable and factual.

---

## âœ¨ Key Features

**ğŸ“¥ Real-Time Ingestion**

- **Kafka-backed** event stream (`live_issues` topic)
- **Fast API** â€” `POST /log_issue` only publishes and returns; no blocking on embedding or indexing
- **Durable queue** â€” Messages buffered if the consumer is down; process when ready
- **Scalable** â€” Multiple producers can log issues; one or more consumers handle indexing

**ğŸ” Hybrid Retrieval Pipeline**

| Step | Component | Purpose |
|------|------------|---------|
| 1 | **Dense (FAISS)** | Semantic search with query embedding â†’ top 5 |
| 2 | **Sparse (BM25)** | Keyword search over document texts â†’ top 5 |
| 3 | **RRF** | Reciprocal Rank Fusion to merge both rankings fairly |
| 4 | **Rerank** | Query-term overlap to boost docs that contain query words |
| 5 | **Top 3** | Final context passed to the LLM for generation |

**ğŸ§  LangGraph Orchestration**

- **Two-node graph**: `retrieve` â†’ `answer`
- **Retrieve node** â€” Loads latest VectorStore from disk, runs dense + BM25 + RRF + rerank, returns top 3 doc strings
- **Answer node** â€” Builds context from docs, calls Groq LLM with an operations-analyst prompt, returns the answer

**ğŸ–¥ï¸ Simple React UI**

- **Ask panel** â€” Type a question, get an answer grounded in live issues
- **Plain-text API** â€” `POST /ask` with body as raw query string
- **CORS** â€” Frontend (Vite dev server) talks to FastAPI on port 8000

---
<p float="left">
  <img src="C:\Users\kriss\OneDrive\Desktop\realtimerag\rag\assets\Screenshot 2026-02-07 115009.png" width="45%" />
  <img src="rag/assets/Screenshot 2026-02-07 115020.png" width="45%" />
</p>

*Backend FastAPI API (left) | Frontend React UI (right)*


## ğŸ—ï¸ Architecture

### System Overview

```
                    INGESTION (async)                    QUERY (sync)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Client  â†’  POST /log_issue  â†’  FastAPI â”‚   â”‚  User  â†’  React  â†’  POST /ask           â”‚
â”‚       â†’  Kafka (live_issues)            â”‚   â”‚       â†’  LangGraph [retrieve â†’ answer]   â”‚
â”‚                                         â”‚   â”‚       â†’  Groq  â†’  answer                 â”‚
â”‚  Consumer  â†  Kafka  â†  embed + FAISS   â”‚   â”‚  retrieve: dense + BM25 + RRF + rerank    â”‚
â”‚       â†’  persist to disk                â”‚   â”‚  answer: context + LLM                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Retrieval Pipeline

```mermaid
graph LR
    A[Query] --> B[Dense FAISS]
    A --> C[BM25 Sparse]
    B --> D[RRF Fusion]
    C --> D
    D --> E[Rerank]
    E --> F[Top 3 Docs]
    F --> G[Answer Node]
    G --> H[Groq LLM]
    H --> I[Response]
```

*(If your GitHub doesn't render Mermaid, the flow is: Query â†’ Dense + BM25 â†’ RRF â†’ Rerank â†’ Top 3 â†’ Answer node â†’ Groq â†’ Response.)*

### Agent Pipeline (LangGraph)

| Node | Input | Output |
|------|--------|--------|
| **retrieve** | `query` | `docs` (top 3 text strings) |
| **answer** | `query`, `docs` | `answer` (LLM response) |

---

## ğŸ› ï¸ Tech Stack

### Frontend

**React** â€¢ **Vite** â€¢ JavaScript (ES modules)

### Backend & AI

- **FastAPI** â€” REST API, CORS, `/ask`, `/log_issue`, `/documents`
- **LangGraph** â€” Retrieve â†’ Answer graph with shared state
- **Sentence Transformers** â€” `all-MiniLM-L6-v2` (384-d) for embeddings
- **FAISS** â€” Vector index (in-memory, persisted to disk)
- **rank_bm25** â€” BM25 sparse retrieval
- **Groq** â€” LLM inference (`llama-3.1-8b-instant`)

### Data & Stream

- **Apache Kafka** (Redpanda) â€” Topic `live_issues`; producer in API, consumer for indexing
- **Persistence** â€” `data/faiss.index`, `data/docs.json`, `data/version.txt`

### Libraries & Tools

- **Uvicorn** â€” ASGI server
- **python-dotenv** â€” Environment variables (e.g. `GROQ_API_KEY`)
- **kafka-python** â€” Producer and consumer clients

---

## ğŸš€ Quick Start

### Prerequisites

- **Python** >= 3.10
- **Node.js** (for frontend)
- **Docker** (for Redpanda / Kafka)
- **Groq API key** â€” [Groq](https://groq.com/)

### Installation

1ï¸âƒ£ **Clone the repository**

```bash
git clone https://github.com/yourusername/realtimerag.git
cd realtimerag/rag
```

2ï¸âƒ£ **Create a virtual environment and install Python dependencies**

```bash
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
pip install fastapi uvicorn langgraph faiss-cpu sentence-transformers groq kafka-python rank-bm25 python-dotenv
```

3ï¸âƒ£ **Set up environment variables**

Create `rag/.env`:

```env
GROQ_API_KEY=your_groq_api_key_here
```

4ï¸âƒ£ **Start Kafka (Redpanda)**

```bash
docker compose -f rag/docker-compose.yaml up -d
```

5ï¸âƒ£ **Start the backend**

```bash
cd rag
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

6ï¸âƒ£ **Start the Kafka consumer** (in a second terminal)

```bash
cd rag
python -m stream.consumer
```

7ï¸âƒ£ **Start the frontend** (in a third terminal)

```bash
cd rag/frontend
npm install
npm run dev
```

8ï¸âƒ£ **Open your browser**

```
http://localhost:5173
```

### Production Build

**Backend:** run with a process manager (e.g. Gunicorn + Uvicorn workers).  
**Frontend:** `cd rag/frontend && npm run build` then serve the `dist/` folder.

---

## ğŸ“š Usage Guide

### 1. Log an issue (ingest)

Send a `POST` request to `/log_issue` (e.g. from a script, monitoring tool, or API client):

```json
{
  "type": "outage",
  "text": "Database replica lag exceeding 5 seconds in region us-east-1",
  "metadata": { "source": "cloudwatch" }
}
```

The API publishes to Kafka and returns immediately. The **consumer** (step 6 above) will embed and index the text so it appears in future answers.

### 2. Ask a question

In the React UI:

- Type your question in the text area (e.g. *"What database issues have been reported?"*).
- Click **Ask**.
- The app calls `POST /ask` with your question as plain text; the backend runs **retrieve** (dense + BM25 + RRF + rerank) then **answer** (Groq), and displays the result.

### 3. List indexed documents (optional)

- **GET** `/documents` returns all documents currently in the store (useful for debugging or inspection).

### 4. Run all three processes

For a full experience:

- **Terminal 1:** `uvicorn app.main:app --reload --port 8000`
- **Terminal 2:** `python -m stream.consumer`
- **Terminal 3:** `cd rag/frontend && npm run dev`

Then log an issue (e.g. with `curl` or Postman) and ask a question in the browserâ€”the new issue should be included in the context for the answer.

---

## ğŸ”Œ API Documentation

### Ask a question

**POST** `/ask`  
**Content-Type:** `text/plain`  
**Body:** Raw string (the question).

**Example:**

```bash
curl -X POST http://127.0.0.1:8000/ask \
  -H "Content-Type: text/plain" \
  -d "What delays or outages were reported today?"
```

**Response:** JSON with the LLM answer.

```json
{
  "answer": "Based on the context, ..."
}
```

*(Exact key may be `answer` or nested under graph output; frontend handles both.)*

---

### Log an issue

**POST** `/log_issue`  
**Content-Type:** `application/json`  
**Body:**

```json
{
  "type": "outage",
  "text": "Description of the issue or alert.",
  "metadata": {}
}
```

**Response:**

```json
{
  "status": "logged",
  "event": {
    "type": "outage",
    "text": "Description of the issue or alert.",
    "metadata": {},
    "timestamp": "2026-02-07T12:00:00.000000"
  }
}
```

---

### List documents

**GET** `/documents`  
**Response:** JSON array of all documents in the current store (for debugging).

---

## ğŸ“ Project Structure

```
realtimerag/
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI app, /documents, /ask, CORS
â”‚   â”‚   â””â”€â”€ issues.py            # POST /log_issue â†’ Kafka producer
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ graph.py             # LangGraph: retrieve â†’ answer
â”‚   â”‚   â”œâ”€â”€ nodes.py             # retrieve_node, answer_node
â”‚   â”‚   â””â”€â”€ state.py             # RAGState (query, docs, answer)
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ embeddings.py        # SentenceTransformer (all-MiniLM-L6-v2)
â”‚   â”‚   â”œâ”€â”€ vector_store.py      # FAISS + DocumentStore, load/save
â”‚   â”‚   â”œâ”€â”€ document_store.py
â”‚   â”‚   â””â”€â”€ persistence.py       # save/load index + docs + version
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â”œâ”€â”€ dense.py             # DenseRetriever (FAISS)
â”‚   â”‚   â”œâ”€â”€ bm.py                # SparseRetriever (BM25)
â”‚   â”‚   â”œâ”€â”€ rrf.py               # RRF fusion
â”‚   â”‚   â””â”€â”€ rerank.py            # simple_rerank (query-word overlap)
â”‚   â”œâ”€â”€ stream/
â”‚   â”‚   â”œâ”€â”€ producer.py          # Kafka producer, send_issue_event()
â”‚   â”‚   â””â”€â”€ consumer.py          # Kafka consumer â†’ add_document()
â”‚   â”œâ”€â”€ data/                    # faiss.index, docs.json, version.txt
â”‚   â”œâ”€â”€ config.py                # EMBEDDING_MODEL, LLM_MODEL, TOP_K
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ docker-compose.yaml      # Redpanda on 9092
â”‚   â””â”€â”€ frontend/                # React + Vite
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ App.jsx
â”‚       â”‚   â”œâ”€â”€ api.js           # askQuestion() â†’ POST /ask
â”‚       â”‚   â””â”€â”€ components/       # Header, AskPanel
â”‚       â””â”€â”€ package.json
â””â”€â”€ README.md                    # This file
```

---

## ğŸ“¤ Pushing to GitHub

Follow these steps to put the project on GitHub (first time).

1. **Initialize git** (from project root `realtimerag/`):

   ```bash
   cd c:\Users\kriss\OneDrive\Desktop\realtimerag
   git init
   ```

2. **Stage and commit**:

   ```bash
   git add .
   git status   # check that .env and meenv/ are not listed
   git commit -m "Initial commit: Real-Time RAG with FastAPI, LangGraph, Kafka, React"
   ```

3. **Create a new repo on GitHub** â€” Go to [github.com/new](https://github.com/new), name it e.g. `realtimerag`, leave it empty (no README/license).

4. **Add remote and push** (replace `YOUR_USERNAME` and `realtimerag` with your repo):

   ```bash
   git remote add origin https://github.com/YOUR_USERNAME/realtimerag.git
   git branch -M main
   git push -u origin main
   ```

If you use SSH: `git remote add origin git@github.com:YOUR_USERNAME/realtimerag.git`

---

## ğŸ¤ Contributing

We welcome contributions. Suggested steps:

1. **Fork** the repository.
2. **Create** a feature branch (`git checkout -b feature/your-feature`).
3. **Commit** your changes (`git commit -m 'Add your feature'`).
4. **Push** to the branch (`git push origin feature/your-feature`).
5. **Open** a Pull Request.

- Keep `.env` and secrets out of commits.
- Add or update tests and docs as needed for new behavior.

---

## ğŸ› Bug Reports & Feature Requests

Open an issue with:

- A **clear title** and description
- **Steps to reproduce** (for bugs)
- **Expected vs actual behavior**
- **Environment** (OS, Python/Node versions)

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€” use and adapt as needed. See the LICENSE file for details.

---

## ğŸ™ Acknowledgments

- **[FastAPI](https://fastapi.tiangolo.com/)** for the modern Python API framework
- **[LangChain / LangGraph](https://langchain-ai.github.io/langgraph/)** for the retrieveâ€“answer orchestration
- **[FAISS](https://github.com/facebookresearch/faiss)** for efficient vector search
- **[Groq](https://groq.com/)** for fast LLM inference
- **[Redpanda](https://redpanda.com/)** for Kafka-compatible streaming
- **Open source community** for the libraries that power this stack

---

### â­ Star us on GitHub â€” it helps others find Real-Time RAG!

**Made with â¤ï¸ for ops and support teams who need answers from live data.**

---

**Real-Time RAG** Â© 2026 â€¢ Ask questions over a live stream of issuesâ€”grounded, up-to-date answers.
